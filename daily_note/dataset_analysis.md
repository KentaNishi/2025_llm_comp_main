# データセット分析レポート

**作成日**: 2026-02-07

## 概要

本プロジェクトには、構造化データ（JSON、XML、YAML、TOML、CSV）の変換・抽出タスクに特化した9つのデータセットが含まれています。これらは2つの主要なグループに分類されます。

## データセット一覧

### グループ1: daichira系（構造化データ変換特化）

#### 1. daichira__structured-3k-mix-sft
- **サンプル数**: 3,000件
- **ファイルサイズ**: 9.3MB
- **ライセンス**: CC-BY-4.0
- **言語**: 英語
- **特徴**:
  - 5つのフォーマット（YAML、TOML、XML、JSON、CSV）に均等配分（各600サンプル、20%）
  - 完全にバランスの取れた分布
  - 複雑な構造化データの変換・抽出タスク
  - 厳格な検証済み（PyYAML、tomllib、ElementTree、csv）

- **タスク内訳**:
  | ターゲット | サンプル数 | 変換元 |
  |-----------|-----------|--------|
  | YAML | 600 | XML/Text/JSON/CSV/TOML |
  | TOML | 600 | Text（深い抽出） |
  | XML | 600 | JSON/CSV |
  | JSON | 600 | 全5フォーマット |
  | CSV | 600 | 全5フォーマット |

#### 2. daichira__structured-5k-mix-sft
- **サンプル数**: 5,000件
- **ファイルサイズ**: 15MB
- **ライセンス**: CC-BY-4.0
- **言語**: 英語
- **特徴**:
  - 13種類の変換タスクを含む
  - フォーマット多様性と構造的複雑性に重点
  - YAML/TOMLに重点配分（各30%）

- **タスク内訳**:
  | ターゲット | サンプル数 | 比率 | タスク種類 |
  |-----------|-----------|------|-----------|
  | YAML | 1,500 | 30% | xml_to_yaml, text_to_yaml, json_to_yaml, csv_to_yaml, toml_to_yaml |
  | TOML | 1,500 | 30% | text_to_toml（抽出重視） |
  | XML | 1,000 | 20% | json_to_xml, csv_to_xml |
  | JSON | 500 | 10% | text_to_json, csv_to_json, xml_to_json, yaml_to_json, toml_to_json |
  | CSV | 500 | 10% | text_to_csv, json_to_csv, xml_to_csv, yaml_to_csv |

#### 3. daichira__structured-hard-sft-4k
- **サンプル数**: 4,000件
- **ファイルサイズ**: 12MB
- **ライセンス**: CC-BY-4.0
- **言語**: 英語
- **特徴**:
  - 高難度タスクのみを厳選
  - 深いネスト構造と多様な型の組み合わせ
  - 完全に合成データで決定論的シリアライゼーション

- **タスク内訳**（各1,000サンプル）:
  | タスク | タイプ | 説明 |
  |-------|--------|------|
  | json_to_xml | 変換 | ネストされたJSON→XML（属性なし、ケース保持） |
  | xml_to_yaml | 変換 | 深くネストされたXML→YAML（タグ名と構造を保持） |
  | text_to_toml | 抽出 | テキストから属性を抽出してTOMLドット表記/AOT |
  | text_to_yaml | 抽出 | テキストから属性を抽出してネストYAML構造 |

### グループ2: u-10bei系（CoT推論付き構造化データ）

#### 4. u-10bei__structured_data_with_cot_dataset
- **サンプル数**: 2,500件
- **ファイルサイズ**: 781KB (Parquet)
- **ライセンス**: MIT
- **言語**: 日本語説明
- **特徴**:
  - Chain-of-Thought（思考連鎖）推論を含む
  - OpenAIチャット形式（system, user, assistant）
  - Fakerライブラリで生成された現実的データ

#### 5. u-10bei__structured_data_with_cot_dataset_512
- **サンプル数**: 3,445件
- **ファイルサイズ**: 1.3MB (Parquet)
- **ライセンス**: MIT
- **特徴**:
  - トークン数を512に制限
  - より多様なプロンプトテンプレート
  - 複雑度の分布を調整（medium、complex多め）
  - 条件付きフィールドを含むスキーマ

#### 6. u-10bei__structured_data_with_cot_dataset_512_v2
- **サンプル数**: 推定約3,500件
- **ファイルサイズ**: 1.6MB (Parquet)
- **ライセンス**: MIT
- **特徴**:
  - v1の改良版
  - プロンプトテンプレートの多様化
  - typeメタデータフィールドの追加

#### 7. u-10bei__structured_data_with_cot_dataset_512_v4
- **サンプル数**: 5,758件（train: 4,608、validation: 575、test: 575）
- **ファイルサイズ**: train=12MB, val/test=各1.5MB
- **ライセンス**: MIT
- **特徴**:
  - train/validation/testに分割済み
  - CoT推論の質向上
  - より詳細なメタデータ

#### 8. u-10bei__structured_data_with_cot_dataset_512_v5
- **サンプル数**: 5,683件（train: 4,547、validation: 568、test: 568）
- **ファイルサイズ**: train=12MB, val/test=各1.5MB
- **ライセンス**: MIT
- **特徴**:
  - **最新版（v5アップデート）**
  - ランダムなスキーマ構造の生成
  - minified（最小化）/sorted（ソート済み）の制約を追加
  - train/validation/testに分割済み

#### 9. u-10bei__structured_data_with_cot_dataset_v2
- **サンプル数**: 2,500件
- **ファイルサイズ**: 853KB (Parquet)
- **ライセンス**: MIT
- **特徴**:
  - オリジナルのv2改良版
  - トークン制限なし版

## データセットの特性比較

### データ構造の違い

**daichira系の構造**:
```json
{
  "id": "ユニークハッシュ",
  "category": "C_TOML",
  "subcategory": "text_to_toml",
  "task": "extract",
  "seed": "dummy_hard",
  "messages": [
    {"role": "user", "content": "..."},
    {"role": "assistant", "content": "..."}
  ]
}
```

**u-10bei系の構造**:
```json
{
  "messages": [
    {"role": "system", "content": "専門家ロール設定"},
    {"role": "user", "content": "プロンプト"},
    {"role": "assistant", "content": "CoT推論 + 出力"}
  ],
  "metadata": {
    "format": "json",
    "complexity": "medium",
    "schema": "research_paper",
    "constraint": null,
    "type": "conversion",
    "estimated_tokens": 149
  }
}
```

### 主要な違い

| 特性 | daichira系 | u-10bei系 |
|-----|-----------|----------|
| **焦点** | 構造化データの変換・抽出の正確性 | CoT推論を伴う構造化データ生成 |
| **systemメッセージ** | なし | あり（専門家ロール） |
| **推論プロセス** | 直接的な変換 | CoT（思考連鎖）を含む |
| **メタデータ** | カテゴリ、タスク、seedのみ | 詳細（format、complexity、schema、tokens） |
| **データ形式** | JSONL | JSONL / Parquet |
| **言語** | 英語 | 日本語説明（データは英語） |
| **サンプル数** | 3k-5k（単一ファイル） | 2.5k-5.7k（分割あり） |
| **ライセンス** | CC-BY-4.0 | MIT |

## サポートされるスキーマ（u-10bei系）

- 研究論文（funding_source等の条件付きフィールド含む）
- 実験結果
- 診療記録
- 検査結果
- 処方箋
- 契約書
- 財務報告書
- 販売分析
- API仕様
- エラーログ
- 製品リスト（is_available等の条件付きフィールド含む）
- 顧客レビュー
- 取引記録
- シラバス
- 学生評価
- ニュース記事
- ソーシャルメディア投稿

## データセット解析方針

### 1. データ品質評価

#### a. 基本統計の取得
- 各データセットのサンプル数、平均トークン数、最大/最小長を計算
- フォーマット別、複雑度別の分布を可視化
- 重複データの有無を確認

#### b. 構造的妥当性の検証
- daichira系: 出力が実際にパース可能か検証
- u-10bei系: CoT推論の質を評価（ステップの明確性、論理性）

### 2. タスク分析

#### a. タスク分類
- **変換タスク** (transformation): format_A → format_B
- **抽出タスク** (extraction): 非構造化テキスト → 構造化データ

#### b. 難易度分析
- ネストの深さ
- データ型の多様性
- スキーマの複雑性

### 3. データセット選択戦略

#### 用途別の推奨

**ケース1: 構造化データ変換の精度向上が目的**
- **推奨**: daichira系全体（12,000サンプル）
- **理由**:
  - 厳格に検証済み
  - タスク分類が明確
  - 高難度タスクを含む

**ケース2: 推論能力の向上が目的**
- **推奨**: u-10bei系の最新版（v5, v4）
- **理由**:
  - CoT推論が含まれる
  - train/validation/testに分割済み
  - より多様な制約条件

**ケース3: バランスの取れた学習**
- **推奨**: 両方を組み合わせる
  - daichira系: 変換精度のベースライン
  - u-10bei系: 推論と解釈の追加

### 4. データセット統合方針

#### a. フォーマット統一
- すべてJSONL形式に変換
- メッセージ構造を統一（OpenAI chat形式）
- メタデータの標準化

#### b. 分割戦略
- **train**: 80%
- **validation**: 10%
- **test**: 10%

u-10bei系でv4/v5を使う場合、既存の分割を維持

#### c. 重複排除
- ID/contentベースのハッシュで重複をチェック
- 類似度の高いサンプルの除外検討

### 5. ファインチューニング戦略

#### a. 段階的学習
1. **Phase 1**: daichira系で基礎的な変換能力を獲得
2. **Phase 2**: u-10bei系のCoTデータで推論能力を強化
3. **Phase 3**: 混合データで汎化性能を向上

#### b. カリキュラム学習
- 複雑度の低いタスクから開始
- 徐々に難易度を上げる
- 最終的に高難度タスク（daichira__structured-hard-sft-4k）で仕上げ

#### c. タスク固有の重み付け
- ターゲットタスクの頻度に応じてサンプリング比率を調整
- 例: JSONが主要なら、JSON関連タスクを多めに

### 6. 評価指標

#### a. 構文的正確性
- パーサーでの検証成功率
- 構文エラーの種類と頻度

#### b. 意味的正確性
- フィールド名の一致率
- データ型の正確性
- ネスト構造の保持率

#### c. CoT品質（u-10bei系）
- 推論ステップの完全性
- 論理的一貫性
- 説明の明確性

### 7. データ拡張の可能性

#### a. 合成データ生成
- 既存のスキーマを組み合わせた新しいタスク
- より深いネスト構造
- より複雑な条件付きロジック

#### b. ノイズ注入
- 軽微な構文エラーを含むデータで堅牢性を向上
- 不完全な入力への対応力強化

## 推奨アクション

1. **即座に実行**:
   - データセット全体のロードとバリデーション
   - 基本統計の取得
   - 重複チェック

2. **短期（1-2日）**:
   - データセットの統合とフォーマット統一
   - train/val/test分割の確定
   - ベースラインモデルでの性能評価

3. **中期（1週間）**:
   - 段階的学習パイプラインの構築
   - カリキュラム学習の実装
   - 評価指標の自動化

4. **長期（継続的）**:
   - データ拡張の実験
   - タスク固有の最適化
   - 新しいスキーマの追加

## 総合的な規模

- **総サンプル数**: 約32,500件
  - daichira系: 12,000件
  - u-10bei系: 約20,500件
- **総ファイルサイズ**: 約70MB（圧縮前）
- **カバー範囲**: JSON、XML、YAML、TOML、CSV の5フォーマット
- **タスク種類**: 20種類以上の変換・抽出パターン

## 結論

このデータセットコレクションは、構造化データ処理の包括的なトレーニングセットとして非常に充実しています。daichira系の厳密性とu-10bei系の推論能力を組み合わせることで、高品質な構造化データ処理モデルの構築が可能です。

**最も重要な判断ポイント**:
- **品質重視**: daichira系をメインに使用
- **推論重視**: u-10bei系v5をメインに使用
- **バランス重視**: 両方を組み合わせて段階的に学習

次のステップとして、実際にデータをロードして分析スクリプトを作成することを推奨します。
