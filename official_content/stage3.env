# 1. モデル・データセット関連
SFT_BASE_MODEL=Qwen/Qwen3-4B-Instruct-2507
SFT_OUT_LORA_DIR=~/workspace/2025_llm_comp_main/output/lora_structeval_t_qwen3_4b_v2
SFT_USE_UPSAMPLING=1
SFT_UPSAMPLE_RULES='{"toml": 2.5, "csv": 1.5}'
SFT_DATASET_ID=u-10bei/structured_data_with_cot_dataset_512_v5

# 複数データセットのMix（オプション）
# 使用例: SFT_DATASET_MIX='[{"id": "dataset1", "weight": 1.0, "split": "train"}, {"id": "dataset2", "weight": 2.0}]'
# SFT_DATASET_MIX=''

# Stage 3
SFT_MAX_SEQ_LEN=1024

SFT_LORA_R=32
SFT_LORA_ALPHA=64
SFT_LORA_DROPOUT=0.05
SFT_LORA_TARGET_MODULES=q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

SFT_EPOCHS=100                # MAX_STEPS で制御するため上限として大きな値を設定
SFT_MAX_STEPS=100             # 実効BS=256 → 元の16×1500÷256 ≈ 93ステップ相当
SFT_PER_DEVICE_TRAIN_BS=16
SFT_GRAD_ACCUM=16             # 実効BS=256

SFT_LR=1.6e-5                 # 線形スケーリング: 1e-6 × (256/16)
SFT_WARMUP_RATIO=0.03
SFT_WEIGHT_DECAY=0.0

SFT_LOGGING_STEPS=5
SFT_EVAL_STEPS=10
SFT_SAVE_STEPS=20
SFT_SAVE_TOTAL_LIMIT=3
SFT_EARLY_STOPPING_PATIENCE=5

SFT_MASK_COT=1
SFT_OUTPUT_MARKERS=Output:,OUTPUT:,Final:,Answer:,Result:,Response:
SFT_OUTPUT_LEARN_MODE=after_marker

# MLflow実験管理（オプション）
# MLFLOW_EXPERIMENT_NAME=llm-training-stage3-v2
