# 1. モデル・データセット関連
SFT_BASE_MODEL=Qwen/Qwen3-4B-Instruct-2507
SFT_DATASET_ID=daichira/structured-5k-mix-sft
SFT_OUT_LORA_DIR=~/workspace/2025_llm_comp_main/output/lora_structeval_t_qwen3_4b

# 複数データセットのMix（オプション）
# 使用例: SFT_DATASET_MIX='[{"id": "dataset1", "weight": 1.0, "split": "train"}, {"id": "dataset2", "weight": 2.0}]'
# SFT_DATASET_MIX=''

# Stage 2
SFT_MAX_SEQ_LEN=2048

SFT_LORA_R=8
SFT_LORA_ALPHA=32
SFT_LORA_DROPOUT=0.05
SFT_LORA_TARGET_MODULES=q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

SFT_EPOCHS=1
SFT_MAX_STEPS=-1              # epochベース
SFT_PER_DEVICE_TRAIN_BS=1
SFT_GRAD_ACCUM=16

SFT_LR=5e-5                   # ステージ1から減衰（過学習と様式崩れ防止）
SFT_WARMUP_RATIO=0.03
SFT_WEIGHT_DECAY=0.0

SFT_MASK_COT=1
SFT_OUTPUT_MARKERS=Output:,OUTPUT:,Final:,Answer:,Result:,Response:
SFT_OUTPUT_LEARN_MODE=after_marker

# ここから"問題形式"を増やす（特にTOML）
SFT_USE_UPSAMPLING=1
SFT_UPSAMPLE_RULES='{"text_to_toml": 3.0, "toml_to_yaml": 2.0, "json_to_toml": 2.0}'

# MLflow実験管理（オプション）
# MLFLOW_EXPERIMENT_NAME=llm-training-stage2
# MLflow UI起動: mlflow ui --backend-store-uri file://~/workspace/2025_llm_comp_main/mlruns --port 58000
